{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append('../volumes/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from time import time, strftime, gmtime\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from prepare_data import *\n",
    "from model_training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "class ModelParameters:\n",
    "    def __init__(self, create_dirs = True):\n",
    "        self.LEARNING_RATE = 0.001\n",
    "        self.DECAY = 0.0\n",
    "        self.LOSS = 'mae'\n",
    "        \n",
    "        self.DEEPANDWIDE = True\n",
    "        \n",
    "        self.NRLAYERS = 3\n",
    "        self.CELLS = [256] * self.NRLAYERS\n",
    "        self.BATCHNORM = [False] * self.NRLAYERS\n",
    "        self.DROPOUT = [0.5] * (self.NRLAYERS) \n",
    "        self.NORMALIZE = True\n",
    "        self.GPUS = None\n",
    "\n",
    "        self.EPOCHS = 5\n",
    "        self.VERBOSE = 0\n",
    "        self.BATCH_SIZE = 2048#512\n",
    "\n",
    "        self.PCA = False\n",
    "        \n",
    "        self.FILENAME = '20200418'\n",
    "        self.STORE_FOLDER = 'DeepAndWide'\n",
    "\n",
    "        self.REDUCE_TRAIN = False\n",
    "        self.LOAD_MODEL = False\n",
    "        self.USE_ATR = True        \n",
    "        \n",
    "        self.update_folders()\n",
    "\n",
    "        if create_dirs:\n",
    "            self.create_dirs()\n",
    "            \n",
    "    def update_folders(self):\n",
    "        self.PATH = './models/{}/'.format(self.STORE_FOLDER)\n",
    "        self.RESULTS_PATH = './results/{}/'.format(self.STORE_FOLDER)\n",
    "        self.FILECORE_PATH = self.PATH + self.FILENAME\n",
    "        self.RESULTS_FILE = self.RESULTS_PATH + self.FILENAME\n",
    "        \n",
    "    def create_dirs(self):\n",
    "        if not os.path.exists(self.PATH):\n",
    "            os.makedirs(self.PATH)\n",
    "        if not os.path.exists(self.RESULTS_PATH):\n",
    "            os.makedirs(self.RESULTS_PATH)       \n",
    "\n",
    "hp = ModelParameters()    \n",
    "\n",
    "    \n",
    "pickle.dump(hp, open('{}_params.p'.format(hp.FILECORE_PATH), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tmc', 'unix_ts', 'date', 'time', 'hour', 'dow',\n",
    "           'tmc_linear', 'tmc_dir', \n",
    "           'miles',\n",
    "           'frc', 'f_system', 'facil_type', 'thru_lanes',\n",
    "           'aadt', 'aadt_single', 'aadt_combi', \n",
    "           'osm_highway', 'osm_lanes',\n",
    "           'speed', 'ref_speed',\n",
    "           'temp_f', 'dew_f', 'rel_humid', 'viz_mi', 'precip1hr_in', \n",
    "           'gps_pt1_wc1', \n",
    "           'gps_pt2_wc1', 'gps_pt2_wc2', 'gps_pt2_wc3', \n",
    "           'tmc_has_gps_data', \n",
    "           'count_type', 'count_subtype', 'count_location', \n",
    "           'count_total',\n",
    "           #'atr_class_volume'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"/hdd3/Volumes/MD2018/ML_data/20200417/MD-2018___CREATED_2020-04-13_142351___ML-INPUTS-COUNT-LOCATION-TMCs.csv\"\n",
    "data_skiprows = None\n",
    "\n",
    "storefile = './tmpdata/2018.p'\n",
    "\n",
    "t = time()\n",
    "\n",
    "# If True data are generated from raw file. Otherwise read from storefile\n",
    "if True: \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(data_filename, skiprows=data_skiprows)\n",
    "    df = df[columns]    \n",
    "        \n",
    "        \n",
    "    if True:\n",
    "        print(\"Deleting weird CCSs counts...\")\n",
    "        df = delete_weird_counts(df, 3.0)\n",
    "\n",
    "    if False:\n",
    "        print ('Adding Hourly Averaged GPS counts....')\n",
    "        df = add_hourly_averaged_gps_counts(df)\n",
    "        \n",
    "    print(\"Preparing data...                                           \")\n",
    "    df = change_values(df)\n",
    "    print ('Before prepare_df', df.shape)\n",
    "    df = prepare_df(df)\n",
    "    print ('Data shape after prepare_df:', df.shape)\n",
    "    \n",
    "    pickle.dump(df, open(storefile, \"wb\"))\n",
    "    \n",
    "else:\n",
    "    df = pickle.load(open(storefile, \"rb\"))\n",
    "\n",
    "    \n",
    "if False: # Q1 data\n",
    "    print ('All data:', df.shape)\n",
    "    df = df[df.datetime < '2018-04-01']     \n",
    "    print ('First Quarter:', df.shape)    \n",
    "\n",
    "    \n",
    "if False: # Q2-4-data    \n",
    "    print ('All data:', df.shape)\n",
    "    df = df[df.datetime >= '2018-04-01']     \n",
    "    print ('Quarters 2-4:', df.shape)    \n",
    "    pickle.dump(df, open(storefile_q234, \"wb\"))\n",
    "    \n",
    "print (\"Data prepared in {:.1f} seconds.\".format(time() - t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning (data splitted previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "print (\"Train test split...\")\n",
    "df_train, df_test = split_train_test_fixed(df)\n",
    "\n",
    "train_X, train_y = get_XY(df_train)\n",
    "train_X, train_y = shuffle(train_X, train_y)   \n",
    "test_X, test_y = get_XY(df_test)\n",
    "\n",
    "print ('train_X shape:', train_X.shape)\n",
    "print ('test_X shape:', test_X.shape)\n",
    "\n",
    "print (\"Data prepared in {:.1f} seconds.\".format(time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp.VERBOSE = 1\n",
    "\n",
    "t = time()\n",
    "\n",
    "if hp.NORMALIZE: \n",
    "    train_X, test_X = normalize(hp, train_X, test_X, save_scaler=True)\n",
    "else:\n",
    "    train_X = train_X.values\n",
    "    test_X = test_X.values\n",
    "    \n",
    "sys.stdout.write('Preparing model...                 \\r')\n",
    "\n",
    "filepath = '{}.hdf5'.format(hp.PATH)\n",
    "\n",
    "model, _ = create_model(hp, train_X.shape[1])\n",
    "\n",
    "filepath_best = '{}_best.hdf5'.format(hp.PATH)\n",
    "checkpoint = ModelCheckpoint(filepath_best, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "\n",
    "print ('EPOCHS: {}                                    '.format(hp.EPOCHS))\n",
    "print (model.summary())\n",
    "print (\"-----\")\n",
    "\n",
    "sys.stdout.write('Fitting model ...                \\r')\n",
    "\n",
    "history = model.fit(train_X, train_y, \n",
    "                    validation_data = (test_X, test_y),\n",
    "                    batch_size = hp.BATCH_SIZE, \n",
    "                    epochs=hp.EPOCHS, \n",
    "                    #epochs=50,\n",
    "                    verbose=hp.VERBOSE)\n",
    "\n",
    "v_loss = history.history['val_loss']\n",
    "t_loss = history.history['loss']\n",
    "\n",
    "\n",
    "sys.stdout.write('Saving model...                \\r')\n",
    "\n",
    "model.save(filepath)\n",
    "\n",
    "sys.stdout.write('Testing model...                \\r')\n",
    "\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "resdf, preddf = generate_results(df_test, pred, verbose=True)\n",
    "\n",
    "plot_train_valid_test(hp, t_loss, v_loss)\n",
    "    \n",
    "print ('Done in {}'.format(strftime('%H:%M:%S', gmtime(time() - t))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean R2 {:.2f}, mean MAPE: {:.1f}%, mean SMAPE: {:.1f}%, mean EMFR: {:.3f}%'\n",
    "      .format(\n",
    "          np.mean(resdf.r2), np.mean(resdf.mape), np.mean(resdf.smape), np.mean(resdf.emfr)\n",
    "      ))\n",
    "\n",
    "print ('Median R2 {:.2f}, median MAPE: {:.1f}%, median SMAPE: {:.1f}%, median EMFR: {:.3f}%'\n",
    "      .format(\n",
    "          np.median(resdf.r2), np.median(resdf.mape), np.median(resdf.smape), np.median(resdf.emfr)\n",
    "      ))\n",
    "\n",
    "print_sumation(resdf)\n",
    "plot_train_valid_test(hp, t_loss, v_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resdf.sort_values(\"r2\").head()\n",
    "#resdf.sort_values(\"mape\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning - Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf, preddf = train_model_cv(hp, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sumation(resdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(hp, preddf, resdf, data_filename, data_skiprows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resdf.sort_values(\"r2\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf, preddf = test_model_cv(hp, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sumation(resdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(hp, preddf, resdf, data_filename, data_skiprows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.FILENAME = 'NoTestData'\n",
    "hp.update_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_XY(df)\n",
    "train_X, train_y = shuffle(train_X, train_y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.VERBOSE = 1\n",
    "t = time()\n",
    "\n",
    "if hp.NORMALIZE:\n",
    "    train_X = normalize(train_X, test=None, save_scaler=True)\n",
    "else:\n",
    "    train_X = train_X.values\n",
    "    \n",
    "sys.stdout.write('Preparing model...                 \\r')\n",
    "\n",
    "filepath = '{}.hdf5'.format(hp.FILECORE_PATH)\n",
    "\n",
    "model, _ = create_model(hp, train_X.shape[1])\n",
    "\n",
    "print ('EPOCHS: {}                                    '.format(hp.EPOCHS))\n",
    "print (model.summary())\n",
    "print (\"-----\")\n",
    "\n",
    "sys.stdout.write('Fitting model ...                \\r')\n",
    "\n",
    "history = model.fit(train_X, train_y, \n",
    "                    batch_size = hp.BATCH_SIZE, \n",
    "                    epochs=hp.EPOCHS, \n",
    "                    verbose=hp.VERBOSE)\n",
    "\n",
    "t_loss = history.history['loss']\n",
    "\n",
    "\n",
    "sys.stdout.write('Saving model...                \\r')\n",
    "\n",
    "model.save(filepath)\n",
    "\n",
    "sys.stdout.write('Testing model...                \\r')\n",
    "\n",
    "plot_train_valid_test(t_loss, t_loss)\n",
    "    \n",
    "print ('Done in {}'.format(strftime('%H:%M:%S', gmtime(time() - t))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
